{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d0b106-f3e0-4936-baff-d178197bc36d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315\n",
      "20\n",
      "37\n",
      "23\n",
      "26\n",
      "12645\n",
      "Data augmentation neighborhoods\n",
      "85261\n",
      "12645\n",
      "Data augmentation Taaleis\n",
      "101160\n",
      "Augmenting data for the ages\n",
      "101160\n",
      "505800\n",
      "505800\n",
      "1011600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "import onnxruntime as rt\n",
    "import onnx\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from skl2onnx import to_onnx\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skl2onnx import convert_sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "import goodModel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed4482d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4471560",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def modify_data(df):\n",
    "    print(\"Starting data augmentation\")\n",
    "\n",
    "    df = goodModel.drop_personality_columns(df)\n",
    "    print(\"Data augmentation dropping personality columns complete\")\n",
    "    df = goodModel.data_augmentation_age(df)\n",
    "    print(\"Data augmentation age complete\")\n",
    "\n",
    "    df = goodModel.drop_taaleis_columns(df)\n",
    "    print(\"Data augmentation dropping taaleis complete\")\n",
    "\n",
    "    df = goodModel.change_labels_data_augmentation_binary(df, 'persoon_geslacht_vrouw')\n",
    "    print(\"Data augmentation gender complete\")\n",
    "\n",
    "\n",
    "    #df = goodModel.data_augmentation_neighborhoods(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d89819-2794-444d-8350-d4e51d1613d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/investigation_train_large_checked.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a8ac592",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data augmentation\n",
      "Data augmentation dropping personality columns complete\n",
      "Data augmentation age complete\n",
      "Data augmentation taaleis complete\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.28 GiB for an array with shape (94, 10400000) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mmodify_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Let's specify the features and the target\u001B[39;00m\n\u001B[0;32m      4\u001B[0m y \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchecked\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "Cell \u001B[1;32mIn[2], line 12\u001B[0m, in \u001B[0;36mmodify_data\u001B[1;34m(df)\u001B[0m\n\u001B[0;32m      9\u001B[0m df \u001B[38;5;241m=\u001B[39m goodModel\u001B[38;5;241m.\u001B[39mdata_augmentation_taaleis(df)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData augmentation taaleis complete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 12\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mgoodModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchange_labels_data_augmentation_binary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpersoon_geslacht_vrouw\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData augmentation gender complete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m#df = goodModel.data_augmentation_neighborhoods(df)\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\Software Testing AI\\repo\\goodModel.py:28\u001B[0m, in \u001B[0;36mchange_labels_data_augmentation_binary\u001B[1;34m(df, feature)\u001B[0m\n\u001B[0;32m     26\u001B[0m selection \u001B[38;5;241m=\u001B[39m resample(df, replace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, n_samples\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(df)), random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m     27\u001B[0m selection[feature] \u001B[38;5;241m=\u001B[39m selection[feature]\u001B[38;5;241m.\u001B[39mreplace([\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m], [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m---> 28\u001B[0m extra_labeled_data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mselection\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m extra_labeled_data\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\Research Project\\repo\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:393\u001B[0m, in \u001B[0;36mconcat\u001B[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[0m\n\u001B[0;32m    378\u001B[0m     copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    380\u001B[0m op \u001B[38;5;241m=\u001B[39m _Concatenator(\n\u001B[0;32m    381\u001B[0m     objs,\n\u001B[0;32m    382\u001B[0m     axis\u001B[38;5;241m=\u001B[39maxis,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    390\u001B[0m     sort\u001B[38;5;241m=\u001B[39msort,\n\u001B[0;32m    391\u001B[0m )\n\u001B[1;32m--> 393\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\Research Project\\repo\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:682\u001B[0m, in \u001B[0;36m_Concatenator.get_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    678\u001B[0m             indexers[ax] \u001B[38;5;241m=\u001B[39m obj_labels\u001B[38;5;241m.\u001B[39mget_indexer(new_labels)\n\u001B[0;32m    680\u001B[0m     mgrs_indexers\u001B[38;5;241m.\u001B[39mappend((obj\u001B[38;5;241m.\u001B[39m_mgr, indexers))\n\u001B[1;32m--> 682\u001B[0m new_data \u001B[38;5;241m=\u001B[39m \u001B[43mconcatenate_managers\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    683\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmgrs_indexers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnew_axes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconcat_axis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbm_axis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\n\u001B[0;32m    684\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    685\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_copy_on_write():\n\u001B[0;32m    686\u001B[0m     new_data\u001B[38;5;241m.\u001B[39m_consolidate_inplace()\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\Research Project\\repo\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:177\u001B[0m, in \u001B[0;36mconcatenate_managers\u001B[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001B[0m\n\u001B[0;32m    167\u001B[0m vals \u001B[38;5;241m=\u001B[39m [ju\u001B[38;5;241m.\u001B[39mblock\u001B[38;5;241m.\u001B[39mvalues \u001B[38;5;28;01mfor\u001B[39;00m ju \u001B[38;5;129;01min\u001B[39;00m join_units]\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m blk\u001B[38;5;241m.\u001B[39mis_extension:\n\u001B[0;32m    170\u001B[0m     \u001B[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001B[39;00m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;66;03m#  we can use np.concatenate, which is more performant\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001B[39;00m\n\u001B[0;32m    176\u001B[0m     \u001B[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001B[39;00m\n\u001B[1;32m--> 177\u001B[0m     values \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_1d_only_ea_dtype(blk\u001B[38;5;241m.\u001B[39mdtype):\n\u001B[0;32m    179\u001B[0m     \u001B[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001B[39;00m\n\u001B[0;32m    180\u001B[0m     values \u001B[38;5;241m=\u001B[39m concat_compat(vals, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, ea_compat_axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 7.28 GiB for an array with shape (94, 10400000) and data type int64"
     ]
    }
   ],
   "source": [
    "data = modify_data(df)\n",
    "\n",
    "# Let's specify the features and the target\n",
    "y = data['checked']\n",
    "X = data.drop(['checked', 'Ja', 'Nee'], axis=1)\n",
    "X = X.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7ba1d-ad88-4d7c-a456-d931f73ce31a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocessing\n",
    "numeric_features = list(range(X.shape[1]))  # assuming all features are numeric in make_classification\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# Combine preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pipelines for both models\n",
    "linreg_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Train both models on full training data\n",
    "linreg_pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f5c30",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pred_to_binary(predictions, threshold=0.5):\n",
    "    \"\"\"Converts risk scores to binary values.\"\"\"\n",
    "    return (predictions >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabfd2b3-4f6b-443c-8c94-fb8fc517e927",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's convert the model to ONNX\n",
    "onnx_model = convert_sklearn(\n",
    "    linreg_pipeline, initial_types=[('X', FloatTensorType((None, X.shape[1])))],\n",
    "    target_opset=12)\n",
    "\n",
    "# How to get metrics such as accuracy\n",
    "sess = rt.InferenceSession(onnx_model.SerializeToString())\n",
    "y_pred_onnx =  sess.run(None, {'X': X_test.values.astype(np.float32)})\n",
    "y_pred_onnx_binary = pred_to_binary(y_pred_onnx[0]) # Do not forget\n",
    "y_test = y_test.astype(int) # Do not forget\n",
    "accuracy_onnx_model = accuracy_score(y_test, y_pred_onnx_binary)\n",
    "\n",
    "print('Accuracy of the ONNX model: ', accuracy_onnx_model)\n",
    "\n",
    "# Let's save the model\n",
    "onnx.save(onnx_model, \"model/model_2.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}