{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import wilcoxon\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from testing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First we load the models and the data \n",
    "data = pd.read_csv('data/investigation_train_large_checked.csv')\n",
    "y = data['checked']\n",
    "X = data.drop(['checked', 'Ja', 'Nee'], axis=1)\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "# model1 = rt.InferenceSession(\"model/model_1.onnx\")\n",
    "model2 = rt.InferenceSession(\"C:\\\\Users\\\\waded\\\\Downloads\\\\group20\\\\model_2\\\\model_2\\\\model_2.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Mutation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1/10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'InferenceSession' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m mutationTestResults \u001B[38;5;241m=\u001B[39m \u001B[43mmutationTest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel1\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel2\u001B[49m\u001B[43m,\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m compareAccuracies(mutationTestResults)\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\Software Testing AI\\repo\\testing.py:21\u001B[0m, in \u001B[0;36mmutationTest\u001B[1;34m(model1, model2, X, y, n_runs)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mRun \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrun\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_runs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     19\u001B[0m _, X_test, _, y_test \u001B[38;5;241m=\u001B[39m train_test_split(X, y, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39mrun)\n\u001B[1;32m---> 21\u001B[0m y_pred_model1 \u001B[38;5;241m=\u001B[39m \u001B[43mmodel1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m(X_test)\n\u001B[0;32m     22\u001B[0m model1_acc \u001B[38;5;241m=\u001B[39m accuracy_score(y_test, y_pred_model1)\n\u001B[0;32m     23\u001B[0m model1_accuracies\u001B[38;5;241m.\u001B[39mappend(model1_acc)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'InferenceSession' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "mutationTestResults = mutationTest(model1,model2,X,y,10)\n",
    "compareAccuracies(mutationTestResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Wilcoxon Test\n",
    "stat, p_value = wilcoxon(mutationTestResults['model1'], mutationTestResults['model1_mutated'])\n",
    "print(\"\\nWilcoxon Test Results for Model 1:\")\n",
    "print(f\"Statistic: {stat:.4f}, P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Significant difference in performance between original and mutated models.\")\n",
    "else:\n",
    "    print(\"No significant difference in performance between original and mutated models.\")\n",
    "\n",
    " # Wilcoxon Test\n",
    "stat, p_value = wilcoxon(mutationTestResults['model2'], mutationTestResults['model2_mutated'])\n",
    "print(\"\\nWilcoxon Test Results for Model 2:\")\n",
    "print(f\"Statistic: {stat:.4f}, P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Significant difference in performance between original and mutated models.\")\n",
    "else:\n",
    "    print(\"No significant difference in performance between original and mutated models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Differentiation Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features_to_modify = ['contacten_soort_document__uitgaand_' , 'afspraak_aantal_woorden' ,'persoon_leeftijd_bij_onderzoek','relatie_kind_leeftijd_verschil_ouder_eerste_kind','persoonlijke_eigenschappen_spreektaal']\n",
    "\n",
    "metrics = differentiationTesting(model1,model2,X,y,0.2,features_to_modify,10)\n",
    "print_average_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Equivalence partitioning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Top 5 worst performing features for model 1: \")\n",
    "model1_results = calculateEPHighestDifference(data,model1)\n",
    "print(model1_results)\n",
    "\n",
    "print(\"Top 5 worst performing features for model 2: \")\n",
    "model2_results = calculateEPHighestDifference(data,model2)\n",
    "print(model2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gender_partitions = [\n",
    "    {\"name\": \"Man\", \"condition\": lambda df: df['persoon_geslacht_vrouw'] < 1},\n",
    "    {\"name\": \"Woman\", \"condition\": lambda df: (df['persoon_geslacht_vrouw'] >= 1)}\n",
    "    \n",
    "]\n",
    "\n",
    "EPResults = equivalencePartitioning(model1,model2,X,y,gender_partitions,1)\n",
    "\n",
    "plot_EP_results(EPResults,'accuracy')\n",
    "plot_EP_results(EPResults,'tp')\n",
    "plot_EP_results(EPResults,'tn')\n",
    "plot_EP_results(EPResults,'fp')\n",
    "plot_EP_results(EPResults,'fn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aitesting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}